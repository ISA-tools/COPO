//this function is called on when the file upload changes, and sets some option in the
//jquery file upload plugin. If the file is larger than chunk_threshold, it should be chunked
var upload_size = 0
var chunk_size = 0
var chunk_threshold = 200000000 // size of chunks in bytes

function get_chunk_size(){
    upload_size = $('#file_upload')[0].files[0].size
    if(upload_size < chunk_threshold){
        chunk_size = 0;
        $('#fileupload').fileupload(
            'option',
            {
                maxChunkSize: 0,
                url: '/rest/receive_data_file/'
            }
        );
    }
    else{
        chunk_size = chunk_threshold
        $('#fileupload').fileupload(
            'option',
            {
                maxChunkSize: chunk_threshold,
                url: '/rest/receive_data_file_chunked/'
            }
        );
    }

}


$(document).ready(function(){

    var u_id = undefined
    var token = $.cookie('csrftoken')
    var f = $('#file_upload')
    $('#upload_id').val('')
    $('#sine_image').hide()
    $('#input_md5_checksum').val('To Be Calculated...')
    $('#upload_info_count').val('0')
    $('#progress').hide()

    $(function () {
    'use strict';
        //file upload plugin
        var s;
        $('#fileupload').fileupload({
            dataType: 'json',
            headers: {'X-CSRFToken':token},
            progressInterval: '300',
            bitrateInterval: '300',
            maxChunkSize: chunk_size,
            singleFileUploads: true,
            fail: function(e, data){
                alert('FAIL!!!')
            },
            done: finalise_upload,
            error: function(e, data){
                console.log($.makeArray(arguments));
            },
            add: function(e, data) {
                data.submit();
            },
            progress: function(e, data){
                //increment progress bar
                var progress = parseInt(data.loaded / data.total * 100, 10);
                $('#progress .bar').css(
                    'width',
                    progress + '%'
                );
                //display uploaded bytes
                var uploaded = parseFloat(data.loaded) / 1000000.0
                s = uploaded.toFixed(2) + " MB of"
                $('#progress_label').html(s)
                //display upload bitrate
                var bit = " @ " + (data.bitrate / 1000.0 / 1000.0 / 8).toFixed(2) + " MB/sec"
                $('#bitrate').html(bit)
            }
        }).on('fileuploadchunkdone', function (e, data) {
            $('#upload_id').val(data.result.upload_id)
            u_id = data.result.upload_id
        }).bind('fileuploadchange', function (e, data) {
            var size = parseFloat(data.files[0].size)
            size = size / 1000000.0
            size = size.toFixed(2) + ' MB'
            $('#total_label').text(size)
            $('#file_upload').addClass()
            $('<div/>').addClass('alert alert-warning').html("<strong>" + data.files[0].name + "</strong>").appendTo($('#file_status_label'));
            $('#upload_files_button').attr('disabled','disabled')
            $('#progress').show()

        })
    })

    //function called to finalised chunked upload
    function finalise_upload(e, data){
            //serialise form
            form = $('#fileupload').serializeFormJSON()
            token = $.cookie('csrftoken')
            var output;
            if(chunk_size > 0){
                //if we have a chunked upload, call the complete view method in django
                $.ajax({
                    headers: {'X-CSRFToken':token},
                    url: "/rest/complete_upload/",
                    type: "POST",
                    data: form,
                    success: update_html,
                    error: function(){
                        alert('error')
                        console.log($.makeArray(arguments));
                    },
                    dataType: 'json'
                })
            }
            else{
                //if we have a non chunked upload, just call the update_html method
                update_html(data)  
            }
    }

    //update the html based on the results of the upload process
    function update_html(data){
        var x;
        if(chunk_size > 0){
            x = $(data.files)
        }
        else{
            x = $(data.result.files)
        }
        $('#file_upload').hide()
        $('div').filter(".alert-warning").hide()
        for(var i = 0; i < x.size(); i++){
            $('<div/>').addClass('alert alert-success').html("<input type='hidden' value='" + x[i].id + "' <ul class='list-inline'><li><strong>" + x[i].name + "</strong></li>-<li>" + x[i].size + " MB</li>").appendTo($('#file_status_label')).fadeIn();
        }

        $('#bitrate').hide()

        $('#file_id').val(x[0].id)

        $('#upload_files_button').removeAttr('disabled')
        $('#progress').hide()

        //now call function to get md5 hash
        get_hash(x[0].id)
    }


    function get_hash(id){
        $('#sine_image').fadeIn()


        $.ajax({ url: "/rest/hash_upload/",
            type: "GET",
            data: {file_id: id},
            dataType: 'application/json'
        }).done(function(data){
            //now find the correct div and append the hash to it
            var obj = jQuery.parseJSON(data)
            $d = $('#' + data.file_id).parent()
            html = '<li><h5><span class="label label-success">' + obj.output_hash + '</span></h5></li>'
            $d.children('.alert').children('ul').append(html)
            $('#sine_image').fadeOut()
        })
    }



    /*
    document.getElementById("file_upload").addEventListener("change", function() {
            var blobSlice = File.prototype.slice || File.prototype.mozSlice || File.prototype.webkitSlice,
                file = this.files[0],
                chunkSize = 2097152,                               // read in chunks of 2MB
                chunks = Math.ceil(file.size / chunkSize),
                currentChunk = 0,
                spark = new SparkMD5.ArrayBuffer(),
                frOnload = function(e) {
                    console.log("read chunk nr", currentChunk + 1, "of", chunks);
                    spark.append(e.target.result);                 // append array buffer
                    currentChunk++;

                    if (currentChunk < chunks) {
                        loadNext();
                    }
                    else {
                       console.log("finished loading");
                       console.info("computed hash", spark.end()); // compute hash
                       $('#md5').val(spark.end())
                    }
                },
                frOnerror = function () {
                    console.warn("oops, something went wrong.");
                };

    function loadNext() {
        var fileReader = new FileReader();
        fileReader.onload = frOnload;
        fileReader.onerror = frOnerror;

        var start = currentChunk * chunkSize,
            end = ((start + chunkSize) >= file.size) ? file.size : start + chunkSize;

        fileReader.readAsArrayBuffer(blobSlice.call(file, start, end));
    };

    loadNext();
    })
    */

})